# Cerebras 

## Connection to a CS-2 node

Connection to one of the CS-2 cluster login nodes requires an MFA passcode for authentication - either an 8-digit passcode generated by an app on your mobile device (e.g. MobilePASS+) or a CRYPTOCard-generated passcode prefixed by a 4-digit pin. This is the same passcode used to authenticate into other ALCF systems, such as Theta and Cooley.

![CS-2 connection diagram](./Cerebras_Wafer-Scale_Cluster_login_diagram.png)

To connect to a CS-2 login, ssh to login nodes:
```bash
ssh ALCFUserID@cerebras.ai.alcf.anl.gov
```

## Prerequisite: Create Virtual Environment 

### PyTorch virtual environment

```bash
#Make your home directory navigable
chmod a+xr ~/
mkdir ~/R_2.1.1
chmod a+x ~/R_2.1.1/
cd ~/R_2.1.1
# Note: "deactivate" does not actually work in scripts.
deactivate
rm -r venv_cerebras_pt
/software/cerebras/python3.8/bin/python3.8 -m venv venv_cerebras_pt
source venv_cerebras_pt/bin/activate
pip install --upgrade pip
pip install cerebras_pytorch==2.1.1
```

## Clone Cerebras modelzoo

We use an example from [Cerebras Modelzoo repository](https://github.com/Cerebras/modelzoo) for this hands-on. 

* Clone the modezoo repository.<br>
```bash
mkdir ~/R_2.1.1
cd ~/R_2.1.1
git clone https://github.com/Cerebras/modelzoo.git
cd modelzoo
git tag
git checkout Release_2.1.1    
```
* Install requirements for modelzoo
    ```bash
    cd ~/R_2.1.1/modelzoo
    pip install -r requirements.txt 
    ```

## Job Queuing and Submission

The CS-2 cluster has its own Kubernetes-based system for job submission and queuing. Jobs are started automatically through the Python scripts. 

Use Cerebras cluster command line tool to get addional information about the jobs.

* Jobs that have not yet completed can be listed as
    `(venv_pt) $ csctl get jobs`
* Jobs can be canceled as shown:
    `(venv_tf) $ csctl cancel job wsjob-eyjapwgnycahq9tus4w7id`

See `csctl -h` for more options.

## Hands-on Example

* [BERT](./bert-large.md)

## Homework

Run BERT example with different batch sizes like 512, 2048 and observe the performance difference.  

BERT with batch size 1024 
![image](https://github.com/user-attachments/assets/4560fc8b-30eb-4924-b33d-09b6dc5c6b6f)

BERT with batch size 512
![image](https://github.com/user-attachments/assets/43299938-3236-4b5c-9f24-9e508774b4fe)

Considering how I get this error when I run the code:
![image](https://github.com/user-attachments/assets/1eb0883d-bcf3-424c-b667-e1d345ffe562)

I am not sure what the actual performance will be but I believe that having larger batch sizes would result in better performance, however, having batch sizes that are too large can also be detrimental because you will do fewer update steps per epoch with a larger batch size.

## Theory Homework

1. What are the key architectural features that make these systems suitable for AI workloads?
   The key architectural features include having specialized hardware that can accelerate matrix multiplications and tensor operations. In addition, they can have higher memory storage and utilization capabilities that are necessary for AI workloads. Lastly, they each have many cores or processing units, and can thus perform calculations in parallel which significantly speeds up training and inference tasks that are asked of it.
   
2. Identify the primary differences between these AI accelerator systems in terms of their architecture and programming models.
    Sambanovas is most suitable for efficient handling of large data due to its Reconfigurable Dataflow Unit (RDU) features a multi-tiered memory architecture with terabytes of addressable memory. Cerebras Wafer-Scale Engine (WSE) consists of processing elements (PEs) with its own memory and operates independently that makes the system highly parallel and scalable. Similarly, Graphcore’s Intelligence Processing Unit (IPU) consists of many interconnected processing tiles, each with its own core and local memory. Because the IPU operates in two phases—computation and communication—using Bulk Synchronous Parallelism (BSP), calculations can be completed much faster. Lastly, Groq’s Tensor Streaming Processor (TSP) focuses on deterministic execution which is suitable for inference tasks where low latency is important.
   
3. Based on hands-on sessions, describe a typical workflow for refactoring an AI model to run on one of ALCF's AI testbeds (e.g., SambaNova or Cerebras). What tools or software stacks are typically used in this process?
  A typical worksflow starts by using vendor specific implementation of a ML framework like PyTorch to port model. Data is then prepared by converting it to the model format and fed into the model to start training it. Input tensors, optimizers, and loss functions are taken into account while training.

4. Give an example of a project that would benefit from AI accelerators and why?
   Any sort of materials discovery would benefit from AI accelerators as there is a multitude of variables that need to be taken into account to successfully theorize then synthesize a new material. The large possibilities creates a large data set that would need parallel computing for efficient time utilization and, a having the ability to make inferences off data and other inputs would widen the scope of possible materials we perhaps have not thought of before.

### Additional Examples (Optional)

* [GPT-J](./gptj.md)
* [GPT-2](./gpt2.md)

# Useful Resources 

* [ALCF Cerebras Documentation](https://docs.alcf.anl.gov/ai-testbed/cerebras/system-overview/)
* [Cerebras Documntation](https://docs.cerebras.net/en/latest/wsc/index.html)
* [Cerebras Modelzoo Repo](https://github.com/Cerebras/modelzoo/tree/main/modelzoo)
* Datasets Path: `/software/cerebras/dataset`
